{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "\n",
    "\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.01\n",
    "LOG_INTERVAL = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...  "
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /kaggle/input/google-quest-challenge/train.csv does not exist: '/kaggle/input/google-quest-challenge/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c55598b7e5ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading data...  '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msample_submission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /kaggle/input/google-quest-challenge/train.csv does not exist: '/kaggle/input/google-quest-challenge/train.csv'"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/kaggle/input/google-quest-challenge'\n",
    "\n",
    "# Load training and testing data\n",
    "print('Loading data...  ', end='')\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "sample_submission_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "print('Done!')\n",
    "\n",
    "# Sort data into separate groups\n",
    "print('Sorting data...  ', end='')\n",
    "train_titles = train_df['question_title'].values\n",
    "train_bodies = train_df['question_body'].values\n",
    "train_answers = train_df['answer'].values\n",
    "\n",
    "test_labels = sample_submission_df.columns\n",
    "test_qa_ids = test_df['qa_id'].values\n",
    "test_titles = test_df['question_title'].values\n",
    "test_bodies = test_df['question_body'].values\n",
    "test_answers = test_df['answer'].values\n",
    "\n",
    "targets = torch.tensor(train_df[train_df.columns[11:]].values, dtype=torch.float)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_unique_words(text):\n",
    "    text = clean(text).lower()\n",
    "    return set(text.split())\n",
    "    \n",
    "def tokenize(text, vocab):\n",
    "    text = clean(text).lower()\n",
    "    return torch.tensor([vocab[w] for w in text.split()], dtype=torch.long)\n",
    "    \n",
    "# Create vocabulary of all words\n",
    "print('Extracting vocabulary words...  ', end='')\n",
    "vocabulary = set()\n",
    "for text in train_titles:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in train_bodies:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in train_answers:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in test_titles:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in test_bodies:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in test_answers:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text)) \n",
    "print(f'Done!')\n",
    "    \n",
    "vocabulary = {word:token for token, word in enumerate(vocabulary)}\n",
    "\n",
    "# Tokenize data\n",
    "print('Tokenizing data...  ', end='')\n",
    "train_title_tokens = []\n",
    "train_body_tokens = []\n",
    "train_answer_tokens = []\n",
    "test_title_tokens = []\n",
    "test_body_tokens = []\n",
    "test_answer_tokens = []\n",
    "for title, body, answer in zip(train_titles, train_bodies, train_answers):\n",
    "    train_title_tokens.append(tokenize(title, vocabulary))\n",
    "    train_body_tokens.append(tokenize(body, vocabulary))\n",
    "    train_answer_tokens.append(tokenize(answer, vocabulary))\n",
    "for title, body, answer in zip(test_titles, test_bodies, test_answers):\n",
    "    test_title_tokens.append(tokenize(title, vocabulary))\n",
    "    test_body_tokens.append(tokenize(body, vocabulary))\n",
    "    test_answer_tokens.append(tokenize(answer, vocabulary))\n",
    "print('Done!')\n",
    "\n",
    "# Get max lengths\n",
    "print('Calculating maximum sequence lengths...  ', end='')\n",
    "max_title_length = max(max([t.size(0) for t in train_title_tokens]), max([t.size(0) for t in test_title_tokens]))\n",
    "max_body_length = max(max([t.size(0) for t in train_body_tokens]), max([t.size(0) for t in test_body_tokens]))\n",
    "max_answer_length = max(max([t.size(0) for t in train_answer_tokens]), max([t.size(0) for t in test_answer_tokens]))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Setting up neural networks...  ', end='')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, embedding_dim=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.embedding(x)\n",
    "        y = self.transformer_encoder(embedded)\n",
    "        padding = torch.zeros(1, self.max_length - y.size(1), y.size(2))\n",
    "        y = torch.cat((y, padding), dim=1).unsqueeze(0)\n",
    "        y = F.relu(F.max_pool2d(self.conv1(y), 2))\n",
    "        y = F.relu(F.max_pool2d(self.conv2(y), 2))\n",
    "        return y\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, title_encoder, body_encoder, answer_encoder):\n",
    "        super(Network, self).__init__()\n",
    "        self.title_encoder = title_encoder\n",
    "        self.body_encoder = body_encoder\n",
    "        self.answer_encoder = answer_encoder\n",
    "        self.conv1 = nn.Conv2d(20, 30, kernel_size=7)\n",
    "        self.conv2 = nn.Conv2d(30, 40, kernel_size=7)\n",
    "        self.conv3 = nn.Conv2d(40, 50, kernel_size=7)\n",
    "        self.lin1 = nn.Linear(56000, 3500)\n",
    "        self.lin2 = nn.Linear(3500, 875)\n",
    "        self.lin3 = nn.Linear(875, 30)\n",
    "    \n",
    "    def forward(self, title, body, answer):\n",
    "        title_y = self.title_encoder(title)\n",
    "        body_y = self.body_encoder(body)\n",
    "        answer_y = self.answer_encoder(answer)\n",
    "        y = torch.cat((title_y, body_y, answer_y), dim=2)\n",
    "        y = F.relu(F.max_pool2d(self.conv1(y), 2))\n",
    "        y = F.relu(F.max_pool2d(self.conv2(y), 2))\n",
    "        y = F.relu(F.max_pool2d(self.conv3(y), 2))\n",
    "        y = y.view(-1, 56000)\n",
    "        y = F.relu(self.lin1(y))\n",
    "        y = F.relu(self.lin2(y))\n",
    "        y = torch.sigmoid(self.lin3(y))\n",
    "        return y.squeeze()\n",
    "    \n",
    "    \n",
    "title_encoder = Encoder(len(vocabulary), max_title_length)\n",
    "body_encoder = Encoder(len(vocabulary), max_body_length)\n",
    "answer_encoder = Encoder(len(vocabulary), max_answer_length)\n",
    "network = Network(title_encoder, body_encoder, answer_encoder)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n--- Begin Training ---')\n",
    "train_data = list(enumerate(zip(train_title_tokens, train_body_tokens, train_answer_tokens, targets)))\n",
    "total = len(train_data)\n",
    "network.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for idx, (title, body, answer, target) in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        output = network(title, body, answer)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch: {epoch:{len(str(EPOCHS))}d}/{EPOCHS}  Loss: {total_loss / (idx + 1):.4f}  Item: {idx + 1:{len(str(total))}d}/{total}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(False)\n",
    "test_data = list(enumerate(zip(test_qa_ids, test_title_tokens, test_body_tokens, test_answer_tokens)))\n",
    "total = len(test_data)\n",
    "output_data = []\n",
    "for idx, (qa_id, title, body, answer) in test_data:\n",
    "    output = network(title, body, answer).tolist()\n",
    "    output.insert(0, qa_id)\n",
    "    output_data.append(output)\n",
    "    print(f'Testing... {idx}/{total}', end='\\r')\n",
    "print('Done!')\n",
    "\n",
    "output_df = pd.DataFrame(output_data, columns=test_labels)\n",
    "output_df.to_csv('submission.csv', index=False)\n",
    "print('Testing output saved to submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
