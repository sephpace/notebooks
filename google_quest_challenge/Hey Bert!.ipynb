{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hey Bert!\n",
    "\n",
    "This is my attempt to teach myself how BERT works.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Install external module(s)\n",
    "\n",
    "import os\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers as trf\n",
    "import yaml\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...  Done!\n",
      "Sorting data...  Done!\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing data\n",
    "DATA_DIR = 'kaggle/input/google-quest-challenge'\n",
    "print('Loading data...  ', end='')\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "sample_submission_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "print('Done!')\n",
    "\n",
    "# Sort data into separate groups\n",
    "print('Sorting data...  ', end='')\n",
    "train_titles = train_df['question_title'].values\n",
    "train_bodies = train_df['question_body'].values\n",
    "train_answers = train_df['answer'].values\n",
    "\n",
    "test_labels = sample_submission_df.columns\n",
    "test_qa_ids = test_df['qa_id'].values\n",
    "test_titles = test_df['question_title'].values\n",
    "test_bodies = test_df['question_body'].values\n",
    "test_answers = test_df['answer'].values\n",
    "\n",
    "targets = torch.tensor(train_df[train_df.columns[11:]].values, dtype=torch.float, device=DEVICE)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452257eaa0c141498b5e6524b67cb376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Help on BertTokenizer in module transformers.tokenization_bert object:\n",
      "\n",
      "class BertTokenizer(transformers.tokenization_utils.PreTrainedTokenizer)\n",
      " |  BertTokenizer(vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, **kwargs)\n",
      " |  \n",
      " |  Constructs a BertTokenizer.\n",
      " |  :class:`~transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
      " |      do_lower_case: Whether to lower case the input. Only has an effect when do_basic_tokenize=True\n",
      " |      do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
      " |      max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n",
      " |          minimum of this value (if specified) and the underlying BERT model's sequence length.\n",
      " |      never_split: List of tokens which will never be split during tokenization. Only has an effect when\n",
      " |          do_basic_tokenize=True\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BertTokenizer\n",
      " |      transformers.tokenization_utils.PreTrainedTokenizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, **kwargs)\n",
      " |      Constructs a BertTokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n",
      " |          **do_lower_case**: (`optional`) boolean (default True)\n",
      " |              Whether to lower case the input\n",
      " |              Only has an effect when do_basic_tokenize=True\n",
      " |          **do_basic_tokenize**: (`optional`) boolean (default True)\n",
      " |              Whether to do basic tokenization before wordpiece.\n",
      " |          **never_split**: (`optional`) list of string\n",
      " |              List of tokens which will never be split during tokenization.\n",
      " |              Only has an effect when do_basic_tokenize=True\n",
      " |          **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
      " |              Whether to tokenize Chinese characters.\n",
      " |              This should likely be deactivated for Japanese:\n",
      " |              see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None)\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
      " |      by concatenating and adding special tokens.\n",
      " |      A BERT sequence has the following format:\n",
      " |          single sequence: [CLS] X [SEP]\n",
      " |          pair of sequences: [CLS] A [SEP] B [SEP]\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens)\n",
      " |      Converts a sequence of tokens (string) in a single string.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None)\n",
      " |      Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
      " |      A BERT sequence pair mask has the following format:\n",
      " |      0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " |      | first sequence    | second sequence\n",
      " |      \n",
      " |      if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False)\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0: list of ids (must not contain special tokens)\n",
      " |          token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
      " |              for sequence pairs\n",
      " |          already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
      " |              special tokens for the model\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  save_vocabulary(self, vocab_path)\n",
      " |      Save the tokenizer vocabulary to a directory or file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  vocab_size\n",
      " |      Size of the base vocabulary (without the added tokens)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  max_model_input_sizes = {'bert-base-cased': 512, 'bert-base-cased-fine...\n",
      " |  \n",
      " |  pretrained_init_configuration = {'bert-base-cased': {'do_lower_case': ...\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'vocab_file': {'bert-base-cased': 'https...\n",
      " |  \n",
      " |  vocab_files_names = {'vocab_file': 'vocab.txt'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Size of the full vocabulary with the added tokens\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict)\n",
      " |      Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n",
      " |      to class attributes. If special tokens are NOT in the vocabulary, they are added\n",
      " |      to it (indexed starting from the last index of the current vocabulary).\n",
      " |      \n",
      " |      Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - special tokens are carefully handled by the tokenizer (they are never split)\n",
      " |      - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be '[CLS]' and XLM's one is also registered to be '</s>')\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:\n",
      " |              [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
      " |              ``additional_special_tokens``].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to add a new classification token to GPT-2\n",
      " |          tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
      " |          model = GPT2Model.from_pretrained('gpt2')\n",
      " |      \n",
      " |          special_tokens_dict = {'cls_token': '<CLS>'}\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
      " |      \n",
      " |          assert tokenizer.cls_token == '<CLS>'\n",
      " |  \n",
      " |  add_tokens(self, new_tokens)\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n",
      " |      vocabulary, they are added to it with indices starting from length of the current vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens: list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs=None, add_special_tokens=False, max_length=None, stride=0, truncation_strategy='longest_first', return_tensors=None, return_input_lengths=False, return_attention_masks=False, **kwargs)\n",
      " |      Returns a dictionary containing the encoded sequence or sequence pair and additional information:\n",
      " |      the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs: Batch of sequences or pair of sequences to be encoded.\n",
      " |              This can be a list of string/string-sequences/int-sequences or a list of pair of\n",
      " |              string/string-sequences/int-sequence (see details in encode_plus)\n",
      " |          add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
      " |              to their model.\n",
      " |          max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n",
      " |              If there are overflowing tokens, those will be added to the returned dictionary`\n",
      " |          stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
      " |              from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |          truncation_strategy: string selected in the following options:\n",
      " |              - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |                  starting from the longest one at each token (when there is a pair of input sequences)\n",
      " |              - 'only_first': Only truncate the first sequence\n",
      " |              - 'only_second': Only truncate the second sequence\n",
      " |              - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |          return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
      " |              or PyTorch torch.Tensor instead of a list of python integers.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids, skip_special_tokens=False)\n",
      " |      Converts a single index or a sequence of indices (integers) in a token \"\n",
      " |      (resp.) a sequence of tokens (str), using the vocabulary and added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens)\n",
      " |      Converts a single token, or a sequence of tokens, (str) in a single integer id\n",
      " |      (resp. a sequence of ids), using the vocabulary.\n",
      " |  \n",
      " |  decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
      " |      Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n",
      " |      with options to remove special tokens and clean up tokenization spaces.\n",
      " |      Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n",
      " |          skip_special_tokens: if set to True, will replace special tokens.\n",
      " |          clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n",
      " |  \n",
      " |  encode(self, text, text_pair=None, add_special_tokens=True, max_length=None, stride=0, truncation_strategy='longest_first', pad_to_max_length=False, return_tensors=None, **kwargs)\n",
      " |      Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method)\n",
      " |          text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      " |              string using the `tokenize` method) or a list of integers (tokenized string ids using the\n",
      " |              `convert_tokens_to_ids` method)\n",
      " |          add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
      " |              to their model.\n",
      " |          max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n",
      " |              If there are overflowing tokens, those will be added to the returned dictionary\n",
      " |          stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
      " |              from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |          truncation_strategy: string selected in the following options:\n",
      " |              - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |                  starting from the longest one at each token (when there is a pair of input sequences)\n",
      " |              - 'only_first': Only truncate the first sequence\n",
      " |              - 'only_second': Only truncate the second sequence\n",
      " |              - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |          pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n",
      " |              padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n",
      " |              The tokenizer padding sides are handled by the class attribute `padding_side` which can be set to the following strings:\n",
      " |              - 'left': pads on the left of the sequences\n",
      " |              - 'right': pads on the right of the sequences\n",
      " |              Defaults to False: no padding.\n",
      " |          return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
      " |              or PyTorch torch.Tensor instead of a list of python integers.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |  \n",
      " |  encode_plus(self, text, text_pair=None, add_special_tokens=True, max_length=None, stride=0, truncation_strategy='longest_first', pad_to_max_length=False, return_tensors=None, return_token_type_ids=True, return_attention_mask=True, return_overflowing_tokens=False, return_special_tokens_mask=False, **kwargs)\n",
      " |      Returns a dictionary containing the encoded sequence or sequence pair and additional informations:\n",
      " |      the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method)\n",
      " |          text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      " |              string using the `tokenize` method) or a list of integers (tokenized string ids using the\n",
      " |              `convert_tokens_to_ids` method)\n",
      " |          add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
      " |              to their model.\n",
      " |          max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n",
      " |              If there are overflowing tokens, those will be added to the returned dictionary\n",
      " |          stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
      " |              from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |          truncation_strategy: string selected in the following options:\n",
      " |              - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |                  starting from the longest one at each token (when there is a pair of input sequences)\n",
      " |              - 'only_first': Only truncate the first sequence\n",
      " |              - 'only_second': Only truncate the second sequence\n",
      " |              - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |          pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n",
      " |              padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n",
      " |              The tokenizer padding sides are handled by the class attribute `padding_side` which can be set to the following strings:\n",
      " |              - 'left': pads on the left of the sequences\n",
      " |              - 'right': pads on the right of the sequences\n",
      " |              Defaults to False: no padding.\n",
      " |          return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
      " |              or PyTorch torch.Tensor instead of a list of python integers.\n",
      " |          return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n",
      " |          return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n",
      " |          return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n",
      " |          return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          A Dictionary of shape::\n",
      " |      \n",
      " |              {\n",
      " |                  input_ids: list[int],\n",
      " |                  token_type_ids: list[int] if return_token_type_ids is True (default)\n",
      " |                  attention_mask: list[int] if return_attention_mask is True (default)\n",
      " |                  overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
      " |                  num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
      " |                  special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
      " |              }\n",
      " |      \n",
      " |          With the fields:\n",
      " |              ``input_ids``: list of token ids to be fed to a model\n",
      " |              ``token_type_ids``: list of token type ids to be fed to a model\n",
      " |              ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n",
      " |              ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n",
      " |              ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n",
      " |              ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n",
      " |              tokens and 1 specifying sequence tokens.\n",
      " |  \n",
      " |  num_added_tokens(self, pair=False)\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      Note:\n",
      " |          This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this\n",
      " |          inside your training loop.\n",
      " |      \n",
      " |      Args:\n",
      " |          pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the\n",
      " |              number of added tokens in the case of a single sequence if set to False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Number of tokens added to sequences\n",
      " |  \n",
      " |  prepare_for_model(self, ids, pair_ids=None, max_length=None, add_special_tokens=True, stride=0, truncation_strategy='longest_first', pad_to_max_length=False, return_tensors=None, return_token_type_ids=True, return_attention_mask=True, return_overflowing_tokens=False, return_special_tokens_mask=False)\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n",
      " |      It adds special tokens, truncates\n",
      " |      sequences if overflowing while taking into account the special tokens and manages a window stride for\n",
      " |      overflowing tokens\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: list of tokenized input ids. Can be obtained from a string by chaining the\n",
      " |              `tokenize` and `convert_tokens_to_ids` methods.\n",
      " |          pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n",
      " |              `tokenize` and `convert_tokens_to_ids` methods.\n",
      " |          max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n",
      " |          add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
      " |              to their model.\n",
      " |          stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n",
      " |              list of inputs.\n",
      " |          truncation_strategy: string selected in the following options:\n",
      " |              - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |                  starting from the longest one at each token (when there is a pair of input sequences)\n",
      " |              - 'only_first': Only truncate the first sequence\n",
      " |              - 'only_second': Only truncate the second sequence\n",
      " |              - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |          pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n",
      " |              padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n",
      " |              The tokenizer padding sides are handled by the following strings:\n",
      " |              - 'left': pads on the left of the sequences\n",
      " |              - 'right': pads on the right of the sequences\n",
      " |              Defaults to False: no padding.\n",
      " |          return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
      " |              or PyTorch torch.Tensor instead of a list of python integers.\n",
      " |          return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n",
      " |          return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n",
      " |          return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n",
      " |          return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n",
      " |      \n",
      " |      Return:\n",
      " |          A Dictionary of shape::\n",
      " |      \n",
      " |              {\n",
      " |                  input_ids: list[int],\n",
      " |                  token_type_ids: list[int] if return_token_type_ids is True (default)\n",
      " |                  overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
      " |                  num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
      " |                  special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
      " |              }\n",
      " |      \n",
      " |          With the fields:\n",
      " |              ``input_ids``: list of token ids to be fed to a model\n",
      " |              ``token_type_ids``: list of token type ids to be fed to a model\n",
      " |      \n",
      " |              ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n",
      " |              ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n",
      " |              ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n",
      " |              tokens and 1 specifying sequence tokens.\n",
      " |  \n",
      " |  save_pretrained(self, save_directory)\n",
      " |      Save the tokenizer vocabulary files together with:\n",
      " |          - added tokens,\n",
      " |          - special-tokens-to-class-attributes-mapping,\n",
      " |          - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n",
      " |      \n",
      " |      This won't save modifications other than (added tokens and special token mapping) you may have\n",
      " |      applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).\n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n",
      " |  \n",
      " |  tokenize(self, text, **kwargs)\n",
      " |      Converts a string in a sequence of tokens (string), using the tokenizer.\n",
      " |      Split in words for word-based vocabulary or sub-words for sub-word-based\n",
      " |      vocabularies (BPE/SentencePieces/WordPieces).\n",
      " |      \n",
      " |      Take care of added tokens.\n",
      " |      \n",
      " |      text: The sequence to be encoded.\n",
      " |      **kwargs: passed to the child `self.tokenize()` method\n",
      " |  \n",
      " |  truncate_sequences(self, ids, pair_ids=None, num_tokens_to_remove=0, truncation_strategy='longest_first', stride=0)\n",
      " |      Truncates a sequence pair in place to the maximum length.\n",
      " |      truncation_strategy: string selected in the following options:\n",
      " |          - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      " |              starting from the longest one at each token (when there is a pair of input sequences).\n",
      " |              Overflowing tokens only contains overflow from the first sequence.\n",
      " |          - 'only_first': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n",
      " |          - 'only_second': Only truncate the second sequence\n",
      " |          - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  from_pretrained(*inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path: either:\n",
      " |      \n",
      " |              - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n",
      " |              - a string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n",
      " |              - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
      " |              - (not applicable to all derived classes, deprecated) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n",
      " |      \n",
      " |          cache_dir: (`optional`) string:\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n",
      " |      \n",
      " |          force_download: (`optional`) boolean, default False:\n",
      " |              Force to (re-)download the vocabulary files and override the cached versions if they exists.\n",
      " |      \n",
      " |          resume_download: (`optional`) boolean, default False:\n",
      " |              Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n",
      " |      \n",
      " |          proxies: (`optional`) dict, default None:\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n",
      " |              The proxies are used on each request.\n",
      " |      \n",
      " |          inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n",
      " |      \n",
      " |          kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n",
      " |      \n",
      " |          # Download vocabulary from S3 and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          # Download vocabulary from S3 (user-uploaded) and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      " |      \n",
      " |          # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      " |      \n",
      " |          # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      " |      \n",
      " |          # You can link tokens to special vocabulary when instantiating\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      " |          # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |          # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |          assert tokenizer.unk_token == '<unk>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string)\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      All the additional special tokens you may want to use (list of strings). Log an error if used while not having been set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set.\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n",
      " |      class attributes (cls_token, unk_token...).\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n",
      " |      (cls_token, unk_token...).\n",
      " |  \n",
      " |  bos_token\n",
      " |      Beginning of sentence token (string). Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      Classification token (string). E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      End of sentence token (string). Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      Id of the end of sentence token in the vocabulary. Log an error if used while not having been set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      Padding token (string). Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      Id of the padding token in the vocabulary. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  sep_token\n",
      " |      Separation token (string). E.g. separate context and query in an input sequence. Log an error if used while not having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n",
      " |      values ('<unk>', '<cls>'...)\n",
      " |  \n",
      " |  unk_token\n",
      " |      Unknown token (string). Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      Id of the unknown token in the vocabulary. Log an error if used while not having been set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = trf.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_unique_words(text):\n",
    "    text = clean(text).lower()\n",
    "    return set(text.split())\n",
    "    \n",
    "def tokenize(text, vocab):\n",
    "    text = clean(text).lower()\n",
    "    tokens = torch.tensor([vocab[w] for w in text.split()], dtype=torch.long, device=DEVICE)\n",
    "    tokens = torch.tensor([vocab['']], device=DEVICE) if tokens.size(0) == 0 else tokens\n",
    "    return tokens\n",
    "    \n",
    "# Create vocabulary of all words\n",
    "print('Extracting vocabulary words...  ', end='')\n",
    "vocabulary = set()\n",
    "for text in train_titles:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in train_bodies:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in train_answers:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in test_titles:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in test_bodies:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text))\n",
    "for text in test_answers:\n",
    "    vocabulary = vocabulary.union(get_unique_words(text)) \n",
    "print(f'Done!')\n",
    "    \n",
    "vocabulary = {word:token for token, word in enumerate(vocabulary, start=1)}\n",
    "vocabulary[''] = 0\n",
    "\n",
    "# Tokenize data\n",
    "print('Tokenizing data...  ', end='')\n",
    "train_title_tokens = []\n",
    "train_body_tokens = []\n",
    "train_answer_tokens = []\n",
    "test_title_tokens = []\n",
    "test_body_tokens = []\n",
    "test_answer_tokens = []\n",
    "for title, body, answer in zip(train_titles, train_bodies, train_answers):\n",
    "    train_title_tokens.append(tokenize(title, vocabulary))\n",
    "    train_body_tokens.append(tokenize(body, vocabulary))\n",
    "    train_answer_tokens.append(tokenize(answer, vocabulary))\n",
    "for title, body, answer in zip(test_titles, test_bodies, test_answers):\n",
    "    test_title_tokens.append(tokenize(title, vocabulary))\n",
    "    test_body_tokens.append(tokenize(body, vocabulary))\n",
    "    test_answer_tokens.append(tokenize(answer, vocabulary))\n",
    "print('Done!')\n",
    "\n",
    "# Get max lengths\n",
    "print('Calculating maximum sequence lengths...  ', end='')\n",
    "max_title_length = max(max([t.size(0) for t in train_title_tokens]), max([t.size(0) for t in test_title_tokens]))\n",
    "max_body_length = max(max([t.size(0) for t in train_body_tokens]), max([t.size(0) for t in test_body_tokens]))\n",
    "max_answer_length = max(max([t.size(0) for t in train_answer_tokens]), max([t.size(0) for t in test_answer_tokens]))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Setting up neural networks...  ', end='')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, embedding_dim=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim).to(device=DEVICE)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8).to(device=DEVICE)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6).to(device=DEVICE)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.embedding(x)\n",
    "        y = self.transformer_encoder(embedded)\n",
    "        padding = torch.zeros(1, self.max_length - y.size(1), y.size(2), device=DEVICE)\n",
    "        y = torch.cat((y, padding), dim=1).unsqueeze(0)\n",
    "        return y\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, title_encoder, body_encoder, answer_encoder, kernel_size=7, pad_size=3):\n",
    "        super(Network, self).__init__()\n",
    "        self.title_encoder = title_encoder\n",
    "        self.body_encoder = body_encoder\n",
    "        self.answer_encoder = answer_encoder\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pad_size = pad_size\n",
    "        \n",
    "        # CNNs\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=self.kernel_size).to(device=DEVICE)\n",
    "        self.conv2 = nn.Conv2d(5, 15, kernel_size=self.kernel_size).to(device=DEVICE)\n",
    "        self.conv3 = nn.Conv2d(15, 30, kernel_size=self.kernel_size).to(device=DEVICE)\n",
    "        self.conv4 = nn.Conv2d(30, 50, kernel_size=self.kernel_size).to(device=DEVICE)\n",
    "        \n",
    "        # Find the size to flatten CNN output\n",
    "        x = torch.rand(1, 1, title_encoder.max_length + body_encoder.max_length + answer_encoder.max_length, 512, device=DEVICE)\n",
    "        y = F.relu(F.max_pool2d(self.conv1(x), self.pad_size))\n",
    "        y = F.relu(F.max_pool2d(self.conv2(y), self.pad_size))\n",
    "        y = F.relu(F.max_pool2d(self.conv3(y), self.pad_size))\n",
    "        y = F.relu(F.max_pool2d(self.conv4(y), self.pad_size))\n",
    "        self.cnn_flatten_size = y.size(1) * y.size(2) * y.size(3)\n",
    "        \n",
    "        # Linear layers\n",
    "        self.lin1 = nn.Linear(self.cnn_flatten_size, 1650).to(device=DEVICE)\n",
    "        self.lin2 = nn.Linear(1650, 412).to(device=DEVICE)\n",
    "        self.lin3 = nn.Linear(412, 30).to(device=DEVICE)\n",
    "    \n",
    "    def forward(self, title, body, answer):\n",
    "        title_y = self.title_encoder(title)\n",
    "        body_y = self.body_encoder(body)\n",
    "        answer_y = self.answer_encoder(answer)\n",
    "        y = torch.cat((title_y, body_y, answer_y), dim=2)\n",
    "        y = F.relu(F.max_pool2d(self.conv1(y), self.pad_size))\n",
    "        y = F.relu(F.max_pool2d(self.conv2(y), self.pad_size))\n",
    "        y = F.relu(F.max_pool2d(self.conv3(y), self.pad_size))\n",
    "        y = F.relu(F.max_pool2d(self.conv4(y), self.pad_size))\n",
    "        y = y.view(-1, self.cnn_flatten_size)\n",
    "        y = F.relu(self.lin1(y))\n",
    "        y = F.relu(self.lin2(y))\n",
    "        y = torch.sigmoid(self.lin3(y))\n",
    "        return y.squeeze()\n",
    "    \n",
    "    \n",
    "title_encoder = Encoder(len(vocabulary), max_title_length)\n",
    "body_encoder = Encoder(len(vocabulary), max_body_length)\n",
    "answer_encoder = Encoder(len(vocabulary), max_answer_length)\n",
    "network = Network(title_encoder, body_encoder, answer_encoder)\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n--- Begin Training ---')\n",
    "train_data = list(enumerate(zip(train_title_tokens, train_body_tokens, train_answer_tokens, targets)))\n",
    "total = len(train_data)\n",
    "network.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for idx, (title, body, answer, target) in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        output = network(title, body, answer)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch: {epoch:{len(str(EPOCHS))}d}/{EPOCHS}  Loss: {total_loss / (idx + 1):.4f}  Item: {idx + 1:{len(str(total))}d}/{total}', end='\\r')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(False)\n",
    "test_data = list(enumerate(zip(test_qa_ids, test_title_tokens, test_body_tokens, test_answer_tokens)))\n",
    "total = len(test_data)\n",
    "output_data = []\n",
    "for idx, (qa_id, title, body, answer) in test_data:\n",
    "    output = network(title, body, answer).tolist()\n",
    "    output.insert(0, qa_id)\n",
    "    output_data.append(output)\n",
    "    print(f'Testing... {idx + 1}/{total}', end='\\r')\n",
    "print('Testing... Done!  ')\n",
    "\n",
    "output_df = pd.DataFrame(output_data, columns=test_labels)\n",
    "output_df.to_csv('submission.csv', index=False)\n",
    "print('Testing output saved to submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
