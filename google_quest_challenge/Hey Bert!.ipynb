{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hey Bert!\n\nThis is my attempt to teach myself how BERT works.\n\n## Imports"},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import DistilBertModel, DistilBertTokenizer, DistilBertConfig\n\n\nif torch.cuda.is_available():\n    DEVICE = torch.device('cuda')\nelse:\n    DEVICE = torch.device('cpu')\n\nDATA_DIR = '/kaggle/input/google-quest-challenge'\nVOCAB_PATH = '/kaggle/input/bertvocab/vocabulary.txt'\nEPOCHS = 2\nLEARNING_RATE = 0.01","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"# Load training and testing data\nprint('Loading data...  ', end='')\ntrain_df = pd.read_csv(f'{DATA_DIR}/train.csv')\ntest_df = pd.read_csv(f'{DATA_DIR}/test.csv')\nsample_submission_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\nprint('Done!')\n\n# Sort data into separate groups\nprint('Sorting data...  ', end='')\ntrain_titles = train_df['question_title'].values\ntrain_bodies = train_df['question_body'].values\ntrain_answers = train_df['answer'].values\n\ntest_labels = sample_submission_df.columns\ntest_qa_ids = test_df['qa_id'].values\ntest_titles = test_df['question_title'].values\ntest_bodies = test_df['question_body'].values\ntest_answers = test_df['answer'].values\n\ntargets = torch.tensor(train_df[train_df.columns[11:]].values, dtype=torch.float, device=DEVICE)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Process Data"},{"metadata":{"pycharm":{"name":"#%%    \n","is_executing":false},"trusted":true},"cell_type":"code","source":"def process_data(title, body, answer, tokenizer):\n    ids = tokenizer.encode(f'[CLS] {title} [SEP] {body} [SEP] {answer} [SEP]')   \n    return torch.tensor(ids, device=DEVICE).unsqueeze(0)\n    \n\n# Get ids, segments, and positions\nprint('Processing data...  ', end='')\ntokenizer = DistilBertTokenizer(vocab_file=VOCAB_PATH)\n\ntrain_ids = []\ntest_ids = []\nfor title, body, answer in zip(train_titles, train_bodies, train_answers):\n    train_ids.append(process_data(title, body, answer, tokenizer))\nfor title, body, answer in zip(test_titles, test_bodies, test_answers):\n    test_ids.append(process_data(title, body, answer, tokenizer))\n    \n# Find max sequence length\nmax_sequence_length = max(max([tid.size(1) for tid in train_ids]), max([tid.size(1) for tid in test_ids]))\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Neural Network"},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":true},"cell_type":"code","source":"print('Setting up neural network...  ', end='')\n\nclass Network(nn.Module):\n    def __init__(self, config, max_length=1024):\n        super(Network, self).__init__()\n        self.max_length = max_length\n        \n        self.bert = DistilBertModel(config).to(device=DEVICE)\n        self.lin1 = nn.Linear(768, 256).to(device=DEVICE)\n        self.lin2 = nn.Linear(256, 64).to(device=DEVICE)\n        self.lin3 = nn.Linear(64, 30).to(device=DEVICE)\n    \n    def forward(self, x):\n        chunks = torch.split(x, self.max_length, dim=1)\n        for chunk in chunks:\n            y = self.bert(chunk)[0][0, 0]\n        y = F.relu(self.lin1(y))\n        y = F.relu(self.lin2(y))\n        y = torch.sigmoid(self.lin3(y))\n        return y.squeeze()\n    \n\nconfig = DistilBertConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=max_sequence_length)\nnetwork = Network(config)\noptimizer = optim.SGD(network.parameters(), lr=LEARNING_RATE)\ncriterion = nn.MSELoss()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"print('\\n--- Begin Training ---')\ntotal = len(train_ids)\nnetwork.train()\nfor epoch in range(1, EPOCHS + 1):\n    total_loss = 0\n    for idx, (ids, target) in enumerate(zip(train_ids, targets)):\n        optimizer.zero_grad()\n        output = network(ids)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        print(f'Epoch: {epoch:{len(str(EPOCHS))}d}/{EPOCHS}  Loss: {total_loss / (idx + 1):.4f}  Item: {idx + 1:{len(str(total))}d}/{total}', end='\\r')\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing"},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"network.train(False)\ntotal = len(test_ids)\noutput_data = []\nfor idx, (qa_id, ids) in enumerate(zip(test_qa_ids, test_ids)):\n    output = network(ids).tolist()\n    output.insert(0, qa_id)\n    output_data.append(output)\n    print(f'Testing... {idx + 1}/{total}', end='\\r')\nprint('Testing... Done!  ')\n\noutput_df = pd.DataFrame(output_data, columns=test_labels)\noutput_df.to_csv('submission.csv', index=False)\nprint('Testing output saved to submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"nbformat":4,"nbformat_minor":1}